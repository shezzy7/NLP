{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Stopwords***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words (like \"the,\" \"is,\" \"in,\" \"and,\" \"of,\" etc.) that appear frequently in a language but usually do not carry significant meaning in Natural Language Processing (NLP) tasks. These words are often removed during text preprocessing to improve efficiency and focus on more meaningful terms.\n",
    "\n",
    "Why Remove Stopwords?\n",
    "\n",
    "Reduce Dimensionality: Eliminating stopwords decreases the number of unique tokens, making models more efficient.\n",
    "\n",
    "Improve Model Performance: Many NLP tasks (like text classification, sentiment analysis) benefit from focusing on more relevant words.\n",
    "\n",
    "Enhance Search Accuracy: In search engines, removing stopwords can improve indexing and retrieval speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for removing stopwords from our data/corpus we can use nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Natural Language Processing (NLP) is a fascinating field of artificial intelligence. \n",
    "It enables computers to understand, interpret, and generate human language. \n",
    "Stopwords are frequently removed during text preprocessing to improve the efficiency of NLP models. \n",
    "Machine learning techniques, along with deep learning, have significantly improved the accuracy of language models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all make tokens of corpus in the form of sentences.\n",
    "words = sent_tokenize(corpus)\n",
    "# also intialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing stopword in a variable for comparing later\n",
    "stpWords = set(stopwords.words('english'))\n",
    "for i in range(len(words)):\n",
    "    # running a loop over all the sentences one by one and then separating all the words of each sentence using word_tokenize.\n",
    "    sentence = nltk.word_tokenize(words[i]) \n",
    "    sentence=[lemmatizer.lemmatize(word,pos=\"v\") for word in sentence if word not in stpWords ] #after tokenizing each words and storing them in a new list we are going to apply lemmatization on them and we will only store only those words that does not come in stopwords set.\n",
    "    words[i] = ' '.join(sentence) #after lemmatization and removing stopwords we are going to replace each tokein in main list words by making a token with these words joing them with a whitespace.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing ( NLP ) fascinate field artificial intelligence . It enable computers understand , interpret , generate human language . Stopwords frequently remove text preprocessing improve efficiency NLP model . Machine learn techniques , along deep learn , significantly improve accuracy language model .\n"
     ]
    }
   ],
   "source": [
    "# Finally we  see that corpus has been processed successfully\n",
    "corpus= ' '.join(words)\n",
    "print(corpus) #now the corpus has lemmatized text with all the stopwords removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
