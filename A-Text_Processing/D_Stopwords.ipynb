{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Stopwords***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are common words (like \"the,\" \"is,\" \"in,\" \"and,\" \"of,\" etc.) that appear frequently in a language but usually do not carry significant meaning in Natural Language Processing (NLP) tasks. These words are often removed during text preprocessing to improve efficiency and focus on more meaningful terms.\n",
    "\n",
    "Why Remove Stopwords?\n",
    "\n",
    "Reduce Dimensionality: Eliminating stopwords decreases the number of unique tokens, making models more efficient.\n",
    "\n",
    "Improve Model Performance: Many NLP tasks (like text classification, sentiment analysis) benefit from focusing on more relevant words.\n",
    "\n",
    "Enhance Search Accuracy: In search engines, removing stopwords can improve indexing and retrieval speed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for removing stopwords from our data/corpus we can use nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"Natural Language Processing (NLP) is a fascinating field of artificial intelligence. \n",
    "It enables computers to understand, interpret, and generate human language. \n",
    "Stopwords are frequently removed during text preprocessing to improve the efficiency of NLP models. \n",
    "Machine learning techniques, along with deep learning, have significantly improved the accuracy of language models.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"wordnet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First of all make tokens of corpus in the form of sentences.\n",
    "words = sent_tokenize(corpus)\n",
    "# also intialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing stopword in a variable for comparing later\n",
    "stpWords = set(stopwords.words('english'))\n",
    "for i in range(len(words)):\n",
    "    # running a loop over all the sentences one by one and then separating all the words of each sentence using word_tokenize.\n",
    "    sentence = nltk.word_tokenize(words[i]) \n",
    "    sentence=[lemmatizer.lemmatize(word,pos=\"v\") for word in sentence if word not in stpWords ] #after tokenizing each words and storing them in a new list we are going to apply lemmatization on them and we will only store only those words that does not come in stopwords set.\n",
    "    words[i] = ' '.join(sentence) #after lemmatization and removing stopwords we are going to replace each tokein in main list words by making a token with these words joing them with a whitespace.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing ( NLP ) fascinate field artificial intelligence . It enable computers understand , interpret , generate human language . Stopwords frequently remove text preprocessing improve efficiency NLP model . Machine learn techniques , along deep learn , significantly improve accuracy language model .\n"
     ]
    }
   ],
   "source": [
    "# Finally we  see that corpus has been processed successfully\n",
    "corpus= ' '.join(words)\n",
    "print(corpus) #now the corpus has lemmatized text with all the stopwords removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Parts of Speech(POS) in NLP***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS refers to the categories of words based on their grammatical properties, such as:\n",
    "\n",
    "1. Nouns (NN): Words that refer to people, places, things, or ideas (e.g., \"dog\", \"city\", \"happiness\").\n",
    "2. Verbs (VB): Words that express actions, events, or states (e.g.,\"run\",\"eat\",\"be\").\n",
    "3. Adjectives (JJ): Words that modify or describe nouns or pronouns (e.g., \"big\", \"happy\", \"blue\").\n",
    "4. Adverbs (RB): Words that modify or describe verbs, adjectives, or other adverbs (e.g., \"quickly\", \"very\", \"well\").\n",
    "5. Pronouns (PRP): Words that replace nouns in a sentence (e.g., \"he\", \"she\", \"it\").\n",
    "6. Prepositions (IN): Words that show relationships between words or phrases (e.g., \"in\", \"on\", \"at\").\n",
    "7. Conjunctions (CC): Words that connect words, phrases, or clauses (e.g., \"and\", \"but\", \"or\").\n",
    "8. Interjections (UH): Words that express emotion or feeling (e.g., \"oh\", \"wow\", \"ouch\").\n",
    "9. Articles (DT): Words that modify nouns and indicate whether they are specific or general (e.g., \"the\", \"a\", \"an\").\n",
    "10. Numbers (CD): Words that express numerical values (e.g., \"one\", \"two\", \"three\").\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has built in methods with help of those we can find POS for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus2 = \"\"\"Natural Language Processing (NLP) is a fascinating field of artificial intelligence. \n",
    "It enables computers to understand, interpret, and generate human language. \n",
    "Stopwords are frequently removed during text preprocessing to improve the efficiency of NLP models. \n",
    "Machine learning techniques, along with deep learning, have significantly improved the accuracy of language models.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', '.', 'It', 'enables', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Stopwords', 'are', 'frequently', 'removed', 'during', 'text', 'preprocessing', 'to', 'improve', 'the', 'efficiency', 'of', 'NLP', 'models', '.', 'Machine', 'learning', 'techniques', ',', 'along', 'with', 'deep', 'learning', ',', 'have', 'significantly', 'improved', 'the', 'accuracy', 'of', 'language', 'models', '.']\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(corpus2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for using pos_tag we need to download following thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input in pos_tag must be a list.On a single word/string it will give an error.So if we want to apply it on single word then we need to put it in a array and pass this array to pos_tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'JJ'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NNP'),\n",
       " (')', ')'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('fascinating', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('artificial', 'JJ'),\n",
       " ('intelligence', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('enables', 'VBZ'),\n",
       " ('computers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('understand', 'VB'),\n",
       " (',', ','),\n",
       " ('interpret', 'VB'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('generate', 'VB'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Stopwords', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('frequently', 'RB'),\n",
       " ('removed', 'VBN'),\n",
       " ('during', 'IN'),\n",
       " ('text', 'JJ'),\n",
       " ('preprocessing', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('improve', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('efficiency', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('models', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Machine', 'NNP'),\n",
       " ('learning', 'VBG'),\n",
       " ('techniques', 'NNS'),\n",
       " (',', ','),\n",
       " ('along', 'IN'),\n",
       " ('with', 'IN'),\n",
       " ('deep', 'JJ'),\n",
       " ('learning', 'NN'),\n",
       " (',', ','),\n",
       " ('have', 'VBP'),\n",
       " ('significantly', 'RB'),\n",
       " ('improved', 'VBN'),\n",
       " ('the', 'DT'),\n",
       " ('accuracy', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('language', 'NN'),\n",
       " ('models', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tokens)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Taj', 'NNP'),\n",
       " ('mahal', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('beautiful', 'JJ'),\n",
       " ('monument', 'NN')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ass = \"Taj mahal is a beautiful monument\"\n",
    "pos_tag(word_tokenize(ass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('heelo', 'NN')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag([\"heelo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
